{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lsJoN4cPSUP"
      },
      "source": [
        "# VGG13 CNN in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu5Lsa3APSUT"
      },
      "source": [
        "## **1 - Preparing the environment**\n",
        "\n",
        "As usual, we will start by loading in the packages. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==1.15.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZkJ8gPp3Qheh",
        "outputId": "2826a382-98d2-42a3-90cd-69cbf88f7e31"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.0.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.44.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.37.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 36.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=ca961d4d183f95dc1c672713dc6720582cf66a467b586dbff44dd71527c6aa37\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z17M0H7bPSUU"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "# tf.logging.set_verbosity(tf.logging.INFO)\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "%matplotlib inline\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Vhjz-TI7RHmR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy15hJqlPSUV"
      },
      "source": [
        "## **2 - Load and preprocess the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUlUEaHOPSUV",
        "outputId": "c66f8ac2-3453-4b31-c969-f2b408b96d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
          ]
        }
      ],
      "source": [
        "# Loading the data (signs)\n",
        "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
        "train_data = mnist.train.images  # Returns np.array\n",
        "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
        "eval_data = mnist.test.images  # Returns np.array\n",
        "eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn30Ff9dPSUW"
      },
      "source": [
        "As a reminder, the MNIST dataset is a collection of images representing numbers from 0 to 9.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "l99UxzTuPSUX",
        "outputId": "665182de-e68e-465c-c875-c6f557c78582"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOpklEQVR4nO3df5BV9XnH8c8jrIAIKQRLt+APgjiRtAlkVjSRNGZsCZI/MJMOIzUGW2fWWmhkEqcl9g9N/+gwHX9PnaQkkhAn6jjxF5nYGGRMGKMhLA6VH2JAgiMEliqmEqj8fPrHHpwV93zvcu8599zd5/2a2bl3z3PPPQ8HPpx7z/ee+zV3F4DB74yqGwDQHIQdCIKwA0EQdiAIwg4EMbSZGzvThvlwjWzmJoFQ3tVBHfHD1letobCb2WxJ90oaIum77r409fjhGqlL7cpGNgkgYa2vzq3V/TLezIZIul/SVZKmSppvZlPrfT4A5WrkPfsMSdvdfYe7H5H0iKS5xbQFoGiNhH2CpDd6/b4rW/Y+ZtZpZl1m1nVUhxvYHIBGlH423t2XuXuHu3e0aVjZmwOQo5Gw75Z0bq/fJ2bLALSgRsK+TtIUM5tkZmdKukbSymLaAlC0uofe3P2YmS2S9Ix6ht6Wu/vmwjoDUKiGxtnd/WlJTxfUC4AS8XFZIAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Jo6pTNaEGXfTxZ/u3Nfc7++57ffHZFsn7hz6/PrU3+mw3JdVEsjuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7IPc3sWfTtb/bdHyZH3WiIPJ+lFPb//eGY/k1u7TR9Mr19D9j+k/258+tDW3dvyt/Q1teyBqKOxmtlPSAUnHJR1z944imgJQvCKO7J9z9zcLeB4AJeI9OxBEo2F3ST8zs/Vm1tnXA8ys08y6zKzrqA43uDkA9Wr0ZfxMd99tZn8saZWZbXX3Nb0f4O7LJC2TpNE2tsbpHABlaejI7u67s9t9kp6QNKOIpgAUr+6wm9lIMxt18r6kWZI2FdUYgGI18jJ+vKQnzOzk8zzk7j8tpCu8jw0blqy/Pe+TubU1t9yZXPcsO7Ounpph1zfS4+jrFt6TrD+6cGJu7b57vpRc95xvv5isD0R1h93dd0j6RIG9ACgRQ29AEIQdCIKwA0EQdiAIwg4EwSWuA8CO2/OH1iRp81f+I1Etd2jt27//SLL+nw9+Ibc2QS8k1z384RPJepsNSdavHbUnt3bJkruS616nryXrA3FojiM7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBOHsLqHUJ68ipbzepkw/6r0OjkvXH/mlWsj7hJ+mx9Kpc1Jb+/MEj37gjWf/89MXp579x3Wn3VDaO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsTWBD07v5tX9NX6++pSN1vXpjOt+4Ilnf96X0OPuw3eWNJ1/wkyPJ+sfPvz5ZX/+pB3Jrta6FnzR0eLI+emtbst6KOLIDQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCMszfB4b+cnqxv+XJ54+g3/+7yZL37C+nx4uNv/a7Idk7LkOdeStbPey69/hOvtufW5p29r56WBrSaR3YzW25m+8xsU69lY81slZlty27HlNsmgEb152X89yXNPmXZEkmr3X2KpNXZ7wBaWM2wu/saSftPWTxX0ors/gpJVxfcF4CC1fuefby7n5xIa6+k8XkPNLNOSZ2SNFxn1bk5AI1q+Gy8u7skT9SXuXuHu3e0Kf3FigDKU2/Yu82sXZKy23inNoEBpt6wr5S0ILu/QNJTxbQDoCw137Ob2cOSrpA0zsx2SbpN0lJJj5rZDZJelzSvzCZbXfdXP52s/8NNT5a6/dRY+m8/m/7//MShU8+9YrCqGXZ3n59TurLgXgCUiI/LAkEQdiAIwg4EQdiBIAg7EASXuPbTGZ+4OLe29Kv5X1ksSVeOONTQtmt93XPqMtXBPLRm0z+WrF/Qlr5ENmX70cPJ+od2HKv7uavCkR0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgmCcvZ8+82D+mG2j4+i1rHvyz5P1CW+9UOr2W9WrN6W/5mzGsNwvUKrpmYNTk/URT/267ueuCkd2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfbMmzd+Klm/acydiWp6pps9x/8vWf/a6+mp8s57vDtZP56sDlxDJ52frP9i9t01nmFE3dt+fv+FNR7xZt3PXRWO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsmQPpIV2dfUZ6LD3ljn2fS2/7M7XGbAfemG4RXl3Ynqy3D6l/HP3tE+8m63vvnZysjxyAfyc1j+xmttzM9pnZpl7Lbjez3Wa2IfuZU26bABrVn5fx35c0u4/ld7v7tOzn6WLbAlC0mmF39zWSBu8cQkAQjZygW2RmL2cv88fkPcjMOs2sy8y6jio9fxaA8tQb9m9JmixpmqQ9knKvEnH3Ze7e4e4dbTUuGAFQnrrC7u7d7n7c3U9I+o6kGcW2BaBodYXdzHqPiXxR0qa8xwJoDTXH2c3sYUlXSBpnZrsk3SbpCjObJskl7ZR0Y4k9Dng/fbYjWZ+kF5vUSYsxS5Z9SHmbvmXXVcn6yB+tLW/jFakZdnef38fiB0roBUCJ+LgsEARhB4Ig7EAQhB0IgrADQXCJaxO0/3KwftlzY/732kuT9a3z7i9t2y/8Mj0l82T9qrRtV4UjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTh7E5x/69ZkvfvHTWqkBEMnTkjWty08L7e29supabClWlNh1/LwgfG5tYu+93Zy3cH4yQiO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBOPsTTDzj7Yn609OuSxZP75tR5HtvM+Qi6ck69sWjEvW7/nr7yXrs0YcTFTLnSFoxcK5ubWhm9eXuu1WxJEdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4JgnD0z5bt7kvVvzpmWW7vtnA3Jdf929BvJ+pCVJ5L1jYcmJuuNmDbyF8n6taPS+6VMKw+OSdZvefaaZP2jv9qcW0vv8cGp5pHdzM41s+fMbIuZbTazm7PlY81slZlty27TfzMAKtWfl/HHJH3d3adKukzSQjObKmmJpNXuPkXS6ux3AC2qZtjdfY+7v5TdPyDpFUkTJM2VtCJ72ApJV5fVJIDGndZ7djO7QNJ0SWsljXf3k2/o9krq8wu/zKxTUqckDddZ9fYJoEH9PhtvZmdLekzSYnd/p3fN3V2S97Weuy9z9w5372gr+cIHAPn6FXYza1NP0H/o7o9ni7vNrD2rt0vaV06LAIpQ82W8mZmkByS94u539SqtlLRA0tLs9qlSOmySYzt2JuvP3Dczt7b4m+npfT90xvBk/SujdyfrqlVvYYf8SG7t/v35w5mStObvLknWL+r6dbIecXgtpT/v2S+XdJ2kjWZ2ckD5VvWE/FEzu0HS65LmldMigCLUDLu7Py/JcspXFtsOgLLwcVkgCMIOBEHYgSAIOxAEYQeCsJ4PvzXHaBvrl9rgO4E/eV16HP3vz/l5sn5xW1uB3TTX/b+fnKw/eO9VubVxy14sup3w1vpqveP7+xw948gOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0HwVdIFeO2Sd5P1JRfOT69//Z8k65+f3ZWs39mefz39x36wKLmuHU+Wa5r80FvJ+rgtjKW3Co7sQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAE17MDgwjXswMg7EAUhB0IgrADQRB2IAjCDgRB2IEgaobdzM41s+fMbIuZbTazm7Plt5vZbjPbkP3MKb9dAPXqz5dXHJP0dXd/ycxGSVpvZquy2t3ufkd57QEoSn/mZ98jaU92/4CZvSJpQtmNASjWab1nN7MLJE2XtDZbtMjMXjaz5WY2JmedTjPrMrOuozrcULMA6tfvsJvZ2ZIek7TY3d+R9C1JkyVNU8+R/86+1nP3Ze7e4e4dbRpWQMsA6tGvsJtZm3qC/kN3f1yS3L3b3Y+7+wlJ35E0o7w2ATSqP2fjTdIDkl5x97t6LW/v9bAvStpUfHsAitKfs/GXS7pO0kYz25Atu1XSfDObJskl7ZR0YykdAihEf87GPy+pr+tjny6+HQBl4RN0QBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIJo6ZbOZ/Y+k13stGifpzaY1cHpatbdW7Uuit3oV2dv57n5OX4Wmhv0DGzfrcveOyhpIaNXeWrUvid7q1azeeBkPBEHYgSCqDvuyiref0qq9tWpfEr3Vqym9VfqeHUDzVH1kB9AkhB0IopKwm9lsM3vVzLab2ZIqeshjZjvNbGM2DXVXxb0sN7N9Zrap17KxZrbKzLZlt33OsVdRby0xjXdimvFK913V0583/T27mQ2R9BtJfyVpl6R1kua7+5amNpLDzHZK6nD3yj+AYWZ/IekPkn7g7n+WLft3SfvdfWn2H+UYd//nFuntdkl/qHoa72y2ovbe04xLulrS9apw3yX6mqcm7LcqjuwzJG139x3ufkTSI5LmVtBHy3P3NZL2n7J4rqQV2f0V6vnH0nQ5vbUEd9/j7i9l9w9IOjnNeKX7LtFXU1QR9gmS3uj1+y611nzvLulnZrbezDqrbqYP4919T3Z/r6TxVTbTh5rTeDfTKdOMt8y+q2f680Zxgu6DZrr7JyVdJWlh9nK1JXnPe7BWGjvt1zTezdLHNOPvqXLf1Tv9eaOqCPtuSef2+n1itqwluPvu7HafpCfUelNRd5+cQTe73VdxP+9ppWm8+5pmXC2w76qc/ryKsK+TNMXMJpnZmZKukbSygj4+wMxGZidOZGYjJc1S601FvVLSguz+AklPVdjL+7TKNN5504yr4n1X+fTn7t70H0lz1HNG/jVJ/1JFDzl9fUTSf2c/m6vuTdLD6nlZd1Q95zZukPRhSaslbZP0rKSxLdTbg5I2SnpZPcFqr6i3mep5if6ypA3Zz5yq912ir6bsNz4uCwTBCTogCMIOBEHYgSAIOxAEYQeCIOxAEIQdCOL/Af7YPko053cVAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Example of a picture\n",
        "train_data.shape\n",
        "index = 7\n",
        "plt.imshow(train_data[index].reshape(28, 28))\n",
        "print (\"y = \" + str(np.squeeze(train_labels[index])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0wY2AqUPSUX",
        "outputId": "13dc9cca-b593-4c7f-fca2-3ffd6a51942b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of training examples = 55000\n",
            "number of evaluation examples = 10000\n",
            "X_train shape: (55000, 784)\n",
            "Y_train shape: (55000,)\n",
            "X_test shape: (10000, 784)\n",
            "Y_test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "# get some statistic from the dataset\n",
        "print (\"number of training examples = \" + str(train_data.shape[0]))\n",
        "print (\"number of evaluation examples = \" + str(eval_data.shape[0]))\n",
        "print (\"X_train shape: \" + str(train_data.shape))\n",
        "print (\"Y_train shape: \" + str(train_labels.shape))\n",
        "print (\"X_test shape: \" + str(eval_data.shape))\n",
        "print (\"Y_test shape: \" + str(eval_labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsgHQJaWPSUY"
      },
      "source": [
        "Let's see one of these 3x3 filters:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh7qSOZDPSUY"
      },
      "source": [
        "## **4 - Implementing the full VGG-13 CNN network:**\n",
        "<br/>\n",
        "<br/>\n",
        "In TensorFlow, there are built-in functions that carry out the convolution steps.\n",
        "\n",
        "- **`tf.layers.conv2d(input, num_filters, filter_size, padding='same', activation=tf.nn.relu)`:** given an input and a group of filters, this function convolves filters on the input.\n",
        "\n",
        "- **`tf.layers.max_ppoling2d(input, pool_size, strides, padding='same')`:** given an input, this function uses a window of size `pool_size` and strides to carry out max pooling over each window.\n",
        "\n",
        "- **`tf.contrib.layers.flatten(P)`**: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k].\n",
        "\n",
        "- **`tf.layers.dense(input, units, activation=tf.nn.relu)`:** given a the flattened input, it returns the output computed using a fully connected layer. The `units' determine the number of neuron in the FC layer.\n",
        "\n",
        "In the last function above (`tf.layers.dense`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, we do not need to initialize those weights.\n",
        "\n",
        "Now we implement the `cnn_model_fn(features, labels, mode)` function below to build the following model: \n",
        "`2 x CONV2D -> RELU -> MAXPOOL -> 2 x CONV2D -> RELU -> MAXPOOL -> 2 x CONV2D -> RELU -> MAXPOOL -> 2 x CONV2D -> RELU -> MAXPOOL -> 2 x CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FC1 -> FC2 -> FC3 -> output`\n",
        "\n",
        "In detail, we will use the following parameters for all the steps:\n",
        "     - Conv2D: A 3 by 3 filter size with stride 1, padding is \"same\" with relu activation function\n",
        "     - Max pool: A 2 by 2 filter size with stride 2, padding is \"same\"\n",
        "     \n",
        "Here is the text version of the network in the original paper:\n",
        "<br/>\n",
        "<img src=\"images/vgg-text.png\" style=\"width:400px;height:400px;\">\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HcyqfLq9PSUZ"
      },
      "outputs": [],
      "source": [
        "def cnn_model_fn(features, labels, mode):\n",
        "    # Input Layer\n",
        "    input_height, input_width = 28, 28\n",
        "    input_channels = 1\n",
        "    input_layer = tf.reshape(features[\"x\"], [-1, input_height, input_width, input_channels])\n",
        "\n",
        "    # Convolutional Layer #1 and Pooling Layer #1\n",
        "    conv1_1 = tf.layers.conv2d(inputs=input_layer, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv1_2 = tf.layers.conv2d(inputs=conv1_1, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool1 = tf.layers.max_pooling2d(inputs=conv1_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "    \n",
        "    # Convolutional Layer #2 and Pooling Layer #2\n",
        "    conv2_1 = tf.layers.conv2d(inputs=pool1, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv2_2 = tf.layers.conv2d(inputs=conv2_1, filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool2 = tf.layers.max_pooling2d(inputs=conv2_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # Convolutional Layer #3 and Pooling Layer #3\n",
        "    conv3_1 = tf.layers.conv2d(inputs=pool2, filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv3_2 = tf.layers.conv2d(inputs=conv3_1, filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool3 = tf.layers.max_pooling2d(inputs=conv3_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # Convolutional Layer #4 and Pooling Layer #4\n",
        "    conv4_1 = tf.layers.conv2d(inputs=pool3, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv4_2 = tf.layers.conv2d(inputs=conv4_1, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool4 = tf.layers.max_pooling2d(inputs=conv4_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # Convolutional Layer #5 and Pooling Layer #5\n",
        "    conv5_1 = tf.layers.conv2d(inputs=pool4, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    conv5_2 = tf.layers.conv2d(inputs=conv5_1, filters=512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
        "    pool5 = tf.layers.max_pooling2d(inputs=conv5_2, pool_size=[2, 2], strides=2, padding=\"same\")\n",
        "\n",
        "    # FC Layers\n",
        "    pool5_flat = tf.contrib.layers.flatten(pool5)\n",
        "    FC1 = tf.layers.dense(inputs=pool5_flat, units=4096, activation=tf.nn.relu)\n",
        "    FC2 = tf.layers.dense(inputs=FC1, units=4096, activation=tf.nn.relu)\n",
        "    FC3 = tf.layers.dense(inputs=FC2, units=1000, activation=tf.nn.relu)\n",
        "\n",
        "    \"\"\"the training argument takes a boolean specifying whether or not the model is currently \n",
        "    being run in training mode; dropout will only be performed if training is true. here, \n",
        "    we check if the mode passed to our model function cnn_model_fn is train mode. \"\"\"\n",
        "    dropout = tf.layers.dropout(inputs=FC3, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
        "    \n",
        "    # Logits Layer or the output layer. which will return the raw values for our predictions.\n",
        "    # Like FC layer, logits layer is another dense layer. We leave the activation function empty \n",
        "    # so we can apply the softmax\n",
        "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
        "    \n",
        "    # Then we make predictions based on raw output\n",
        "    predictions = {\n",
        "        # Generate predictions (for PREDICT and EVAL mode)\n",
        "        # the predicted class for each example - a vlaue from 0-9\n",
        "        \"classes\": tf.argmax(input=logits, axis=1),\n",
        "        # to calculate the probablities for each target class we use the softmax\n",
        "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "    }\n",
        "    \n",
        "    # so now our predictions are compiled in a dict object in python and using that we return an estimator object\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
        "    \n",
        "    \n",
        "    '''Calculate Loss (for both TRAIN and EVAL modes): computes the softmax entropy loss. \n",
        "    This function both computes the softmax activation function as well as the resulting loss.'''\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\n",
        "    # Configure the Training Options (for TRAIN mode)\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
        "        \n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "    # Add evaluation metrics (for EVAL mode)\n",
        "    eval_metric_ops = {\n",
        "        \"accuracy\": tf.metrics.accuracy(labels=labels,\n",
        "                                        predictions=predictions[\"classes\"])}\n",
        "    return tf.estimator.EstimatorSpec(mode=mode,\n",
        "                                      loss=loss,\n",
        "                                      eval_metric_ops=eval_metric_ops)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6hH4nWuPSUa"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhFO1GuaPSUa",
        "outputId": "cba35f11-e23e-4d13-b1d0-e7652c561fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/mnist_vgg13_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0ea688f6d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ]
        }
      ],
      "source": [
        "# Create the Estimator\n",
        "mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
        "                                          model_dir=\"/tmp/mnist_vgg13_model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": train_data},\n",
        "                                                    y=train_labels,\n",
        "                                                    batch_size=100,\n",
        "                                                    num_epochs=3, #100\n",
        "                                                    shuffle=True)\n",
        "mnist_classifier.train(input_fn=train_input_fn,\n",
        "                       steps=None,\n",
        "                       hooks=None)"
      ],
      "metadata": {
        "id": "SxoeUkyxgK-_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and print results\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": eval_data},\n",
        "                                                   y=eval_labels,\n",
        "                                                   num_epochs=1,\n",
        "                                                   shuffle=False)\n",
        "eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "5kKvccuxgGl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "convolutional-neural-networks",
      "graded_item_id": "bwbJV",
      "launcher_item_id": "0TkXB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "AI_Specialist_Support_07C_(VGG13_from_Scratch_with_TensorFlow).ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}